{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import argparse\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (MLP) model.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Size of the input layer.\n",
    "        hidden_size (int): Size of the hidden layers.\n",
    "        output_size (int): Size of the output layer.\n",
    "        num_layers (int): Number of hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=16, output_size=10, num_layers=3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.append(nn.Sequential(nn.Linear(input_size, hidden_size, bias=False), nn.ReLU()))\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.Sequential(nn.Linear(hidden_size, hidden_size, bias=False), nn.ReLU()))\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(iter(self.parameters())).device\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    DataLoader class to handle dataset loading and transformations.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load.\n",
    "        train_size (int): Size of the training dataset.\n",
    "        train_batch_size (int): Batch size for training.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_name='MNIST', train_size=10000, train_batch_size=64):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.train_size = train_size\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.5,), (0.5,))])\n",
    "        self.name2dataset = {\n",
    "            'MNIST': datasets.MNIST,\n",
    "            'FashionMNIST': datasets.FashionMNIST,\n",
    "            'CIFAR10': datasets.CIFAR10,\n",
    "            'CIFAR100': datasets.CIFAR100,\n",
    "        }\n",
    "        self.train_dataset = self.name2dataset[self.dataset_name](f'~/.pytorch/{self.dataset_name}_data/',\n",
    "                                                                  download=True, train=True, transform=self.transform)\n",
    "        self.train_dataset = torch.utils.data.Subset(self.train_dataset, np.arange(self.train_size))\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(self.train_dataset, batch_size=self.train_batch_size, shuffle=True)\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    ModelTrainer class to handle model training and evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        dataloader (DataLoader): The dataloader containing data.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        device (torch.device): The device to run the model on.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, dataloader, criterion):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.criterion = criterion\n",
    "        self.device = model.device\n",
    "\n",
    "    def criterion_params(self, params, x, y):\n",
    "        \"\"\"\n",
    "        Calculate the loss using the given parameters.\n",
    "\n",
    "        Args:\n",
    "            params (list): List of parameters.\n",
    "            x (torch.Tensor): Input data.\n",
    "            y (torch.Tensor): Target data.\n",
    "\n",
    "        Returns:\n",
    "            float: The calculated loss.\n",
    "        \"\"\"\n",
    "        names = list(n for n, _ in self.model.named_parameters())\n",
    "        output = torch.func.functional_call(self.model, {n: p for n, p in zip(names, params)}, x)\n",
    "        loss = self.criterion(output, y)\n",
    "        return loss\n",
    "\n",
    "    def get_loss_abs_differences(self, loss_values, shuffle=True):\n",
    "        \"\"\"\n",
    "        Calculate the absolute differences of the loss values.\n",
    "\n",
    "        Args:\n",
    "            loss_values (list): List of loss values.\n",
    "            shuffle (bool): Whether to shuffle the loss values.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The absolute differences of the loss values.\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            loss_values = random.sample(loss_values, len(loss_values))\n",
    "        loss_cumsum_values = np.cumsum(loss_values)\n",
    "        loss_mean_values = loss_cumsum_values / np.arange(1, len(loss_cumsum_values) + 1)\n",
    "        loss_abs_differences = abs(np.diff(loss_mean_values))\n",
    "        return loss_abs_differences\n",
    "\n",
    "    def calculate_ema(self, data, window=10):\n",
    "        \"\"\"\n",
    "        Calculate the Exponential Moving Average (EMA) of the data.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): The data to calculate the EMA for.\n",
    "            window (int): The window size for the EMA.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The calculated EMA.\n",
    "        \"\"\"\n",
    "        weights = np.exp(np.linspace(-1., 0., window))\n",
    "        weights /= weights.sum()\n",
    "        ema = np.convolve(data, weights, mode='full')[:len(data)]\n",
    "        ema[:window] = ema[window]\n",
    "        return ema\n",
    "\n",
    "    def create_random_parameters(self):\n",
    "        \"\"\"\n",
    "        Create random parameters for the model.\n",
    "\n",
    "        Returns:\n",
    "            list: List of random parameters.\n",
    "        \"\"\"\n",
    "        random_params = []\n",
    "        for param in self.model.parameters():\n",
    "            random_params.append(nn.Parameter(torch.randn_like(param)))\n",
    "        return random_params\n",
    "\n",
    "    def calculate_delta(self, B=10, k=100):\n",
    "        \"\"\"\n",
    "        Calculate the \\Delta_k using Monte-Carlo approximation.\n",
    "\n",
    "        Args:\n",
    "            B (int): Number of sample points for Monte-Carlo approximation.\n",
    "            k (int): Number of samples.\n",
    "\n",
    "        Returns:\n",
    "            float: The mean of the absolute differences of the loss values.\n",
    "        \"\"\"\n",
    "        abs_values = []\n",
    "\n",
    "        for _ in tqdm(range(B)):\n",
    "            random_params = self.create_random_parameters()\n",
    "            loss_values = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for idx, (x, y) in enumerate(self.dataloader.train_dataloader.dataset):\n",
    "                    x, y = x.to(self.device), torch.tensor([y]).to(self.device)\n",
    "                    x = x.view(-1, self.model.input_size)\n",
    "                    loss = self.criterion_params(random_params, x, y).item()\n",
    "                    loss_values.append(loss)\n",
    "                    if idx + 1 == k + 1:\n",
    "                        break\n",
    "\n",
    "            loss_abs_differences = self.get_loss_abs_differences(loss_values, shuffle=False)\n",
    "            abs_values.append(loss_abs_differences[k-1])\n",
    "\n",
    "        delta = np.mean(abs_values)\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Configuration\n",
    "config = OmegaConf.load('../configs/delta_experiments.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "model = MLP(**config.model).to(device)\n",
    "\n",
    "# Initialize DataLoader\n",
    "dataloader = DataLoader(**config.data)\n",
    "\n",
    "# Initialize Criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize ModelTrainer\n",
    "model_trainer = ModelTrainer(model, dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the delta\n",
    "delta = model_trainer.calculate_delta(B=10, k=100)\n",
    "delta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkiselev_landscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
